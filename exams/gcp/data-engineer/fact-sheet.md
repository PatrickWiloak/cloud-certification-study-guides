# Google Cloud Professional Data Engineer - Fact Sheet

## Quick Reference

**Exam Code:** Professional Data Engineer
**Duration:** 120 minutes (2 hours)
**Questions:** 50-60 questions
**Passing Score:** ~70% (not officially published)
**Cost:** $200 USD
**Validity:** 2 years
**Difficulty:** â­â­â­â­â­ (Expert-level data engineering certification)
**Prerequisites:** Recommended 3+ years of industry experience, including 1+ year designing and managing data solutions on GCP

## Exam Domains

| Domain | Weight | Key Focus |
|--------|--------|-----------|
| Designing data processing systems | 22% | Storage systems, data pipelines, processing infrastructure |
| Ingesting and processing data | 25% | Data ingestion, transformation, ETL/ELT pipelines |
| Storing data | 20% | Database selection, schema design, data lifecycle |
| Preparing and using data for analysis | 15% | Data preparation, visualization, ML integration |
| Maintaining and automating data workloads | 18% | Monitoring, optimization, testing, automation |

## Core Data Engineering Principles

### Data Engineering Framework

**[ğŸ“– Data Engineering on Google Cloud](https://cloud.google.com/architecture/data-engineering)** - Complete data engineering guide

**Key Pillars:**
1. **Data Ingestion** - Batch and streaming data collection
2. **Data Processing** - ETL/ELT transformations
3. **Data Storage** - Appropriate storage for use case
4. **Data Analysis** - Analytics and ML preparation
5. **Data Governance** - Security, compliance, quality

**Essential Resources:**
- **[ğŸ“– Data Lifecycle on Google Cloud](https://cloud.google.com/architecture/data-lifecycle-cloud-platform)** - End-to-end data lifecycle
- **[ğŸ“– Big Data Reference Architectures](https://cloud.google.com/architecture/reference-patterns/overview)** - Common patterns
- **[ğŸ“– Data Analytics Best Practices](https://cloud.google.com/architecture/best-practices-for-data-analytics)** - Analytics guidelines
- **[ğŸ“– Modern Data Architecture](https://cloud.google.com/blog/topics/developers-practitioners/smart-analytics-reference-patterns-part-1)** - Architecture patterns

## Data Ingestion Services

### Cloud Pub/Sub

**Event Streaming and Messaging:**
- Global message queue for real-time and batch scenarios
- At-least-once delivery guarantee
- Message ordering within a single publisher
- Pull and push subscriptions
- Message retention: Default 7 days, up to 31 days
- Dead letter topics for failed messages
- **[ğŸ“– Pub/Sub Overview](https://cloud.google.com/pubsub/docs/overview)** - Architecture and concepts
- **[ğŸ“– Publisher and Subscriber](https://cloud.google.com/pubsub/docs/publisher)** - Publishing and consuming
- **[ğŸ“– Message Ordering](https://cloud.google.com/pubsub/docs/ordering)** - Guaranteed ordering
- **[ğŸ“– Schema Validation](https://cloud.google.com/pubsub/docs/schemas)** - Message validation
- **[ğŸ“– Dead Letter Topics](https://cloud.google.com/pubsub/docs/dead-letter-topics)** - Error handling
- **[ğŸ“– Pub/Sub Lite](https://cloud.google.com/pubsub/lite/docs)** - Cost-optimized regional messaging

**Advanced Features:**
- **Exactly-once delivery** - Deduplication for subscriptions
- **Message filtering** - Subscriber-side message filtering
- **Seek and replay** - Replay messages from specific points
- **Snapshots** - Capture subscription state for replay
- **[ğŸ“– Exactly-Once Delivery](https://cloud.google.com/pubsub/docs/exactly-once-delivery)** - Deduplication
- **[ğŸ“– Best Practices](https://cloud.google.com/pubsub/docs/best-practices)** - Performance optimization

### Storage Transfer Service

**Data Transfer Solutions:**
- Transfer data from AWS S3, Azure Blob, HTTP/HTTPS sources
- Scheduled and one-time transfers
- Bandwidth throttling and filtering
- **[ğŸ“– Storage Transfer Service](https://cloud.google.com/storage-transfer/docs)** - Overview and setup
- **[ğŸ“– Transfer from AWS](https://cloud.google.com/storage-transfer/docs/create-transfers)** - AWS S3 migration
- **[ğŸ“– Transfer Appliance](https://cloud.google.com/transfer-appliance/docs)** - Offline data transfer for petabyte-scale

### BigQuery Data Transfer Service

**Automated Data Imports:**
- Scheduled imports from SaaS applications (Google Ads, YouTube, etc.)
- Partner integrations (Amazon S3, Teradata, Redshift)
- Automated backfill and incremental updates
- **[ğŸ“– BigQuery Data Transfer Service](https://cloud.google.com/bigquery-transfer/docs)** - Complete guide
- **[ğŸ“– Supported Data Sources](https://cloud.google.com/bigquery-transfer/docs/introduction)** - Available connectors

### Cloud Data Fusion

**Visual ETL Pipeline Builder:**
- No-code/low-code data integration
- Pre-built connectors for 150+ sources
- Visual pipeline designer with drag-and-drop
- CDAP (Cask Data Application Platform) based
- **[ğŸ“– Cloud Data Fusion](https://cloud.google.com/data-fusion/docs)** - Overview and architecture
- **[ğŸ“– Pipeline Creation](https://cloud.google.com/data-fusion/docs/how-to/create-pipelines)** - Building pipelines
- **[ğŸ“– Wrangler](https://cloud.google.com/data-fusion/docs/concepts/wrangler)** - Interactive data preparation

## Data Processing Services

### Dataflow

**Unified Stream and Batch Processing:**
- Apache Beam-based serverless processing
- Auto-scaling from zero to thousands of workers
- Exactly-once processing semantics
- Late data handling with windowing and triggers
- Flexible Resource Scheduling (FlexRS) for batch cost savings
- **[ğŸ“– Dataflow Overview](https://cloud.google.com/dataflow/docs/concepts/overview)** - Architecture fundamentals
- **[ğŸ“– Apache Beam Programming Guide](https://beam.apache.org/documentation/programming-guide/)** - Beam concepts
- **[ğŸ“– Dataflow Templates](https://cloud.google.com/dataflow/docs/concepts/dataflow-templates)** - Pre-built templates
- **[ğŸ“– Streaming Engine](https://cloud.google.com/dataflow/docs/guides/deploying-a-pipeline#streaming-engine)** - Streaming optimization
- **[ğŸ“– Windowing](https://cloud.google.com/dataflow/docs/concepts/streaming-pipelines#windowing)** - Time-based grouping
- **[ğŸ“– Triggers and Watermarks](https://cloud.google.com/dataflow/docs/concepts/streaming-pipelines#triggers)** - Late data handling

**Advanced Patterns:**
- **Side inputs** - Broadcast data to all workers
- **State and Timers** - Stateful processing
- **Splittable DoFn** - Efficiently process large elements
- **[ğŸ“– Dataflow Best Practices](https://cloud.google.com/dataflow/docs/guides/best-practices)** - Performance optimization
- **[ğŸ“– Dataflow SQL](https://cloud.google.com/dataflow/docs/guides/sql/dataflow-sql-intro)** - SQL-based pipelines
- **[ğŸ“– FlexRS](https://cloud.google.com/dataflow/docs/guides/flexrs)** - Cost-optimized batch processing

### Dataproc

**Managed Hadoop and Spark:**
- Fast cluster provisioning (90 seconds)
- Autoscaling clusters based on workload
- Preemptible VMs for worker nodes (80% cost savings)
- Component gateway for web UIs
- Job scheduling and workflow management
- **[ğŸ“– Dataproc Overview](https://cloud.google.com/dataproc/docs/concepts/overview)** - Architecture and use cases
- **[ğŸ“– Cluster Creation](https://cloud.google.com/dataproc/docs/guides/create-cluster)** - Cluster configuration
- **[ğŸ“– Autoscaling](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/autoscaling)** - Dynamic scaling
- **[ğŸ“– Workflow Templates](https://cloud.google.com/dataproc/docs/concepts/workflows/overview)** - Job orchestration
- **[ğŸ“– Initialization Actions](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/init-actions)** - Custom cluster setup

**Ecosystem Support:**
- Apache Spark, Hadoop, Pig, Hive, Presto
- Jupyter and Zeppelin notebooks
- **Serverless Spark** - Fully managed Spark without clusters
- **[ğŸ“– Dataproc Jobs](https://cloud.google.com/dataproc/docs/guides/submit-job)** - Job submission
- **[ğŸ“– Dataproc Hub](https://cloud.google.com/dataproc/docs/concepts/dataproc-hub)** - Notebook-based workflows
- **[ğŸ“– Serverless Spark](https://cloud.google.com/dataproc-serverless/docs)** - Serverless batch processing
- **[ğŸ“– Metastore](https://cloud.google.com/dataproc-metastore/docs)** - Managed Hive metastore

**Optimization Strategies:**
- Preemptible workers for cost optimization
- Enhanced flexibility mode for reliability
- Local SSDs for shuffle operations
- **[ğŸ“– Best Practices](https://cloud.google.com/dataproc/docs/concepts/dataproc-best-practices)** - Performance guidelines

### Cloud Composer

**Managed Apache Airflow:**
- Workflow orchestration for data pipelines
- DAG (Directed Acyclic Graph) based workflows
- Rich operator ecosystem (GCP, AWS, Azure)
- Built-in monitoring and logging
- Environment versioning and upgrades
- **[ğŸ“– Cloud Composer Overview](https://cloud.google.com/composer/docs/concepts/overview)** - Architecture fundamentals
- **[ğŸ“– Creating DAGs](https://cloud.google.com/composer/docs/how-to/using/writing-dags)** - Workflow creation
- **[ğŸ“– GCP Operators](https://cloud.google.com/composer/docs/how-to/using/writing-dags#airflow-operators)** - GCP integrations
- **[ğŸ“– Composer Environments](https://cloud.google.com/composer/docs/composer-2/composer-versioning-overview)** - Version management
- **[ğŸ“– Best Practices](https://cloud.google.com/composer/docs/composer-2/optimize-environments)** - Performance optimization

**Common Patterns:**
- Data pipeline orchestration (ETL/ELT)
- Cross-cloud workflow coordination
- ML pipeline automation
- Data validation and quality checks
- **[ğŸ“– Triggering DAGs](https://cloud.google.com/composer/docs/how-to/using/triggering-with-gcf)** - Event-driven workflows

### Dataprep by Trifacta

**Visual Data Preparation:**
- Interactive UI for data exploration
- Smart suggestions for data transformations
- No-code data wrangling
- Automatic data profiling and statistics
- **[ğŸ“– Dataprep Documentation](https://cloud.google.com/dataprep/docs)** - Complete guide
- **[ğŸ“– Recipe Creation](https://cloud.google.com/dataprep/docs/html/Recipes-Page_57344842)** - Building transformations

## Data Storage Services

### BigQuery

**Serverless Data Warehouse:**
- Petabyte-scale SQL analytics
- Separation of compute and storage
- Standard SQL (ANSI 2011) support
- Automatic backup and disaster recovery
- Integration with BI tools (Looker, Tableau, Power BI)
- **[ğŸ“– BigQuery Overview](https://cloud.google.com/bigquery/docs/introduction)** - Architecture fundamentals
- **[ğŸ“– BigQuery Storage](https://cloud.google.com/bigquery/docs/storage_overview)** - Columnar storage
- **[ğŸ“– Query Execution](https://cloud.google.com/bigquery/docs/query-overview)** - Query processing
- **[ğŸ“– Slots and Reservations](https://cloud.google.com/bigquery/docs/slots)** - Capacity management
- **[ğŸ“– Pricing Models](https://cloud.google.com/bigquery/pricing)** - On-demand vs flat-rate

**Table Design and Optimization:**
- **Partitioning** - Time-based, integer range, or ingestion time
- **Clustering** - Sort data within partitions (up to 4 columns)
- **Nested and Repeated Fields** - STRUCT and ARRAY types
- **Search Indexes** - Full-text search capabilities
- **[ğŸ“– Partitioned Tables](https://cloud.google.com/bigquery/docs/partitioned-tables)** - Partition strategies
- **[ğŸ“– Clustered Tables](https://cloud.google.com/bigquery/docs/clustered-tables)** - Clustering design
- **[ğŸ“– Nested and Repeated Data](https://cloud.google.com/bigquery/docs/nested-repeated)** - Complex data types
- **[ğŸ“– Search Indexes](https://cloud.google.com/bigquery/docs/search-intro)** - Text search optimization

**Advanced Features:**
- **Materialized Views** - Pre-computed query results
- **BI Engine** - In-memory analytics acceleration
- **External Tables** - Query data in Cloud Storage, Bigtable, Sheets
- **Federated Queries** - Query across Cloud SQL, Spanner
- **Wildcard Tables** - Query multiple tables with patterns
- **[ğŸ“– Materialized Views](https://cloud.google.com/bigquery/docs/materialized-views-intro)** - View optimization
- **[ğŸ“– BI Engine](https://cloud.google.com/bigquery/docs/bi-engine-intro)** - Query acceleration
- **[ğŸ“– External Tables](https://cloud.google.com/bigquery/docs/external-tables)** - External data sources
- **[ğŸ“– Federated Queries](https://cloud.google.com/bigquery/docs/federated-queries-intro)** - Cross-service queries

**Data Loading Patterns:**
- Batch loading (CSV, JSON, Avro, Parquet, ORC)
- Streaming inserts via API
- Storage Write API for high-throughput streaming
- BigQuery Data Transfer Service
- **[ğŸ“– Loading Data](https://cloud.google.com/bigquery/docs/loading-data)** - Ingestion methods
- **[ğŸ“– Streaming Inserts](https://cloud.google.com/bigquery/docs/streaming-data-into-bigquery)** - Real-time loading
- **[ğŸ“– Storage Write API](https://cloud.google.com/bigquery/docs/write-api)** - High-throughput streaming

**Cost Optimization:**
- Query cost: $5 per TB processed (on-demand)
- Use partitioning and clustering to reduce scanned data
- Preview queries to estimate cost
- Set maximum bytes billed limits
- Use flat-rate pricing for predictable costs
- **[ğŸ“– Cost Optimization](https://cloud.google.com/bigquery/docs/best-practices-costs)** - Cost reduction strategies
- **[ğŸ“– Query Optimization](https://cloud.google.com/bigquery/docs/best-practices-performance-overview)** - Performance tuning

### Cloud Bigtable

**Wide-Column NoSQL Database:**
- Petabyte-scale, sub-10ms latency
- Ideal for time-series, IoT, financial, AdTech
- HBase API compatible (easy migration)
- Linear scalability with node count
- Multi-cluster replication for HA and geo-distribution
- **[ğŸ“– Bigtable Overview](https://cloud.google.com/bigtable/docs/overview)** - Architecture and use cases
- **[ğŸ“– Storage Model](https://cloud.google.com/bigtable/docs/storage-model)** - Data organization
- **[ğŸ“– Instances and Clusters](https://cloud.google.com/bigtable/docs/instances-clusters-nodes)** - Infrastructure setup

**Schema Design Best Practices:**
- Row key design is critical (determines performance)
- Avoid monotonically increasing keys (hotspots)
- Reverse timestamps for time-series data
- Field promotion for frequently accessed data
- Keep column families small
- **[ğŸ“– Schema Design](https://cloud.google.com/bigtable/docs/schema-design)** - Design patterns
- **[ğŸ“– Schema Design for Time-Series](https://cloud.google.com/bigtable/docs/schema-design-time-series)** - Time-series patterns
- **[ğŸ“– Row Key Design](https://cloud.google.com/bigtable/docs/choosing-a-row-key)** - Performance optimization

**Performance and Scaling:**
- Autoscaling based on CPU and storage utilization
- Replication for read scaling and disaster recovery
- App profiles for traffic routing
- **[ğŸ“– Performance Tuning](https://cloud.google.com/bigtable/docs/performance)** - Optimization guide
- **[ğŸ“– Replication](https://cloud.google.com/bigtable/docs/replication-overview)** - Multi-cluster setup
- **[ğŸ“– App Profiles](https://cloud.google.com/bigtable/docs/app-profiles)** - Traffic management

**Integration with Data Processing:**
- Native Dataflow connectors
- Hadoop/Spark integration via HBase API
- **[ğŸ“– Dataflow Integration](https://cloud.google.com/bigtable/docs/dataflow-hbase)** - Streaming pipelines
- **[ğŸ“– Beam Connector](https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/gcp/bigtable/BigtableIO.html)** - Apache Beam integration

### Cloud Storage

**Object Storage for Data Lakes:**
- Unlimited scale, 11 9's durability
- Storage classes: Standard, Nearline, Coldline, Archive
- Lifecycle management for automatic tiering
- Versioning and retention policies
- Strong consistency for all operations
- **[ğŸ“– Cloud Storage Overview](https://cloud.google.com/storage/docs/introduction)** - Complete guide
- **[ğŸ“– Storage Classes](https://cloud.google.com/storage/docs/storage-classes)** - Class comparison
- **[ğŸ“– Lifecycle Management](https://cloud.google.com/storage/docs/lifecycle)** - Automated policies
- **[ğŸ“– Object Versioning](https://cloud.google.com/storage/docs/object-versioning)** - Version control

**Data Lake Patterns:**
- Raw zone â†’ Processed zone â†’ Curated zone
- Partition data by date in folder structure
- Use Parquet or Avro for analytics workloads
- Compress data to reduce storage and transfer costs
- **[ğŸ“– Data Lake Best Practices](https://cloud.google.com/architecture/build-a-data-lake-on-gcp)** - Architecture patterns
- **[ğŸ“– Object Notification](https://cloud.google.com/storage/docs/pubsub-notifications)** - Event-driven processing

**Performance Optimization:**
- Parallel composite uploads for large files
- Regional vs multi-regional for latency/availability trade-offs
- Request rate limits: 5000 writes per second per prefix
- **[ğŸ“– Request Rate and Access Distribution](https://cloud.google.com/storage/docs/request-rate)** - Performance guidelines
- **[ğŸ“– Turbo Replication](https://cloud.google.com/storage/docs/turbo-replication)** - Fast geo-replication

### Cloud SQL

**Managed Relational Databases:**
- MySQL, PostgreSQL, SQL Server support
- High availability with automatic failover
- Read replicas for scaling reads
- Point-in-time recovery
- Integration with Dataflow, Dataproc, BigQuery
- **[ğŸ“– Cloud SQL Overview](https://cloud.google.com/sql/docs/introduction)** - Architecture guide
- **[ğŸ“– High Availability](https://cloud.google.com/sql/docs/mysql/high-availability)** - HA configuration
- **[ğŸ“– Replication](https://cloud.google.com/sql/docs/mysql/replication)** - Read replicas and cross-region
- **[ğŸ“– Backup and Recovery](https://cloud.google.com/sql/docs/mysql/backup-recovery/backups)** - Backup strategies

**Data Engineering Use Cases:**
- Operational data store for applications
- ETL source and target
- Metadata storage for pipelines
- Federation target from BigQuery
- **[ğŸ“– Federated Queries from BigQuery](https://cloud.google.com/bigquery/docs/cloud-sql-federated-queries)** - Cross-service queries

### Cloud Spanner

**Globally Distributed SQL Database:**
- Horizontal scalability with ACID transactions
- 99.999% availability SLA (multi-region)
- Automatic sharding and replication
- Strong consistency globally
- SQL support with extensions
- **[ğŸ“– Cloud Spanner Overview](https://cloud.google.com/spanner/docs/overview)** - Architecture concepts
- **[ğŸ“– Schema Design](https://cloud.google.com/spanner/docs/schema-design)** - Best practices
- **[ğŸ“– Replication](https://cloud.google.com/spanner/docs/replication)** - Multi-region setup
- **[ğŸ“– Query Best Practices](https://cloud.google.com/spanner/docs/sql-best-practices)** - Performance optimization

**Data Engineering Integration:**
- Dataflow connector for batch and streaming
- Federated queries from BigQuery
- Change streams for CDC pipelines
- **[ğŸ“– Dataflow Integration](https://cloud.google.com/spanner/docs/dataflow-connector)** - Pipeline integration
- **[ğŸ“– Change Streams](https://cloud.google.com/spanner/docs/change-streams)** - Change data capture

### Firestore

**NoSQL Document Database:**
- Serverless with automatic scaling
- Real-time synchronization
- Offline support for mobile/web
- ACID transactions
- Native and Datastore modes
- **[ğŸ“– Firestore Overview](https://cloud.google.com/firestore/docs)** - Complete guide
- **[ğŸ“– Data Model](https://cloud.google.com/firestore/docs/data-model)** - Documents and collections
- **[ğŸ“– Queries](https://cloud.google.com/firestore/docs/query-data/queries)** - Query capabilities
- **[ğŸ“– Dataflow Integration](https://cloud.google.com/dataflow/docs/guides/templates/provided-streaming#firestoretobigtable)** - Streaming pipelines

### Memorystore

**Managed In-Memory Databases:**
- Memorystore for Redis: Caching, session store, pub/sub
- Memorystore for Memcached: High-performance caching
- Sub-millisecond latency
- High availability with automatic failover
- **[ğŸ“– Memorystore for Redis](https://cloud.google.com/memorystore/docs/redis)** - Redis overview
- **[ğŸ“– Memorystore for Memcached](https://cloud.google.com/memorystore/docs/memcached)** - Memcached overview

## Machine Learning and AI Services

### Vertex AI

**Unified ML Platform:**
- AutoML for no-code model training
- Custom training with pre-built containers
- Model deployment and serving (online/batch)
- Feature Store for feature management
- ML Pipelines for workflow automation
- **[ğŸ“– Vertex AI Overview](https://cloud.google.com/vertex-ai/docs/start/introduction-unified-platform)** - Platform overview
- **[ğŸ“– AutoML](https://cloud.google.com/vertex-ai/docs/beginner/beginners-guide)** - Automated ML
- **[ğŸ“– Custom Training](https://cloud.google.com/vertex-ai/docs/training/custom-training)** - Custom models
- **[ğŸ“– Feature Store](https://cloud.google.com/vertex-ai/docs/featurestore/overview)** - Feature management
- **[ğŸ“– ML Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines/introduction)** - Workflow orchestration
- **[ğŸ“– Model Monitoring](https://cloud.google.com/vertex-ai/docs/model-monitoring/overview)** - Drift detection

**Vertex AI Workbench:**
- Jupyter notebook environment
- Pre-configured for ML frameworks
- Integration with Git and Docker
- **[ğŸ“– Vertex AI Workbench](https://cloud.google.com/vertex-ai/docs/workbench/introduction)** - Notebook environment

### BigQuery ML

**SQL-Based Machine Learning:**
- Train models using SQL (no Python required)
- Supported models: Linear/logistic regression, K-means, time-series, AutoML
- Model export to Vertex AI for deployment
- Integration with TensorFlow and XGBoost
- **[ğŸ“– BigQuery ML Overview](https://cloud.google.com/bigquery-ml/docs/introduction)** - BQML fundamentals
- **[ğŸ“– Model Types](https://cloud.google.com/bigquery-ml/docs/introduction#supported_models)** - Available algorithms
- **[ğŸ“– CREATE MODEL](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-create)** - Model training syntax
- **[ğŸ“– Model Evaluation](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-evaluate)** - Model assessment
- **[ğŸ“– Hyperparameter Tuning](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-hp-tuning-overview)** - Optimization

**Common Use Cases:**
- Customer churn prediction
- Product recommendations
- Demand forecasting
- Anomaly detection
- **[ğŸ“– BQML Tutorials](https://cloud.google.com/bigquery-ml/docs/tutorials)** - Hands-on examples

### Pre-trained AI APIs

**Ready-to-Use AI Services:**
- Vision API: Image analysis and OCR
- Natural Language API: Text analysis and entity extraction
- Translation API: Language translation
- Speech-to-Text and Text-to-Speech
- Video Intelligence API: Video analysis
- **[ğŸ“– Vision API](https://cloud.google.com/vision/docs)** - Image analysis
- **[ğŸ“– Natural Language API](https://cloud.google.com/natural-language/docs)** - Text understanding
- **[ğŸ“– Translation API](https://cloud.google.com/translate/docs)** - Language translation
- **[ğŸ“– Speech-to-Text](https://cloud.google.com/speech-to-text/docs)** - Audio transcription
- **[ğŸ“– Video Intelligence](https://cloud.google.com/video-intelligence/docs)** - Video analysis

### TensorFlow and AI Platform

**Deep Learning Infrastructure:**
- TensorFlow Enterprise with Long-Term Support
- Distributed training with GPUs/TPUs
- TensorFlow Extended (TFX) for production ML pipelines
- **[ğŸ“– TensorFlow Enterprise](https://cloud.google.com/tensorflow-enterprise)** - Enterprise TF
- **[ğŸ“– Cloud TPU](https://cloud.google.com/tpu/docs)** - Tensor Processing Units

## Data Governance and Security

### Data Catalog

**Metadata Management:**
- Automatic metadata discovery for BigQuery, Pub/Sub
- Custom metadata with tags and templates
- Data lineage tracking
- Search and discovery across data assets
- **[ğŸ“– Data Catalog Overview](https://cloud.google.com/data-catalog/docs/concepts/overview)** - Metadata management
- **[ğŸ“– Tag Templates](https://cloud.google.com/data-catalog/docs/concepts/overview#tag_templates)** - Custom metadata
- **[ğŸ“– Data Lineage](https://cloud.google.com/data-catalog/docs/concepts/about-data-lineage)** - Lineage tracking

### Data Loss Prevention (DLP)

**Sensitive Data Discovery and Protection:**
- Automatic PII detection (credit cards, SSN, emails)
- Custom info types for organization-specific data
- De-identification techniques (masking, tokenization, encryption)
- Inspection templates and job triggers
- **[ğŸ“– DLP Overview](https://cloud.google.com/dlp/docs/dlp-overview)** - DLP fundamentals
- **[ğŸ“– Inspecting Data](https://cloud.google.com/dlp/docs/inspecting-storage)** - Data inspection
- **[ğŸ“– De-identification](https://cloud.google.com/dlp/docs/deidentify-sensitive-data)** - Data protection
- **[ğŸ“– InfoTypes](https://cloud.google.com/dlp/docs/infotypes-reference)** - Detectable data types

### Identity and Access Management (IAM)

**Data Access Control:**
- Resource-level permissions (project, dataset, table)
- Column-level security in BigQuery
- Row-level security with authorized views
- VPC Service Controls for data perimeter security
- **[ğŸ“– IAM Overview](https://cloud.google.com/iam/docs/overview)** - IAM fundamentals
- **[ğŸ“– BigQuery IAM](https://cloud.google.com/bigquery/docs/access-control)** - Dataset permissions
- **[ğŸ“– Column-Level Security](https://cloud.google.com/bigquery/docs/column-level-security)** - Fine-grained access
- **[ğŸ“– Row-Level Security](https://cloud.google.com/bigquery/docs/row-level-security-intro)** - Row filtering
- **[ğŸ“– VPC Service Controls](https://cloud.google.com/vpc-service-controls/docs)** - Data exfiltration protection

### Encryption and Key Management

**Data Protection:**
- Default encryption at rest (Google-managed keys)
- Customer-Managed Encryption Keys (CMEK) with Cloud KMS
- Customer-Supplied Encryption Keys (CSEK)
- Encryption in transit (TLS)
- **[ğŸ“– Encryption at Rest](https://cloud.google.com/docs/security/encryption/default-encryption)** - Default protection
- **[ğŸ“– Cloud KMS](https://cloud.google.com/kms/docs)** - Key management
- **[ğŸ“– CMEK](https://cloud.google.com/kms/docs/cmek)** - Customer-managed keys
- **[ğŸ“– BigQuery Encryption](https://cloud.google.com/bigquery/docs/encryption-at-rest)** - BQ encryption options

### Data Quality and Validation

**Ensuring Data Integrity:**
- Great Expectations integration with Dataflow
- Dataprep data quality rules
- BigQuery data quality checks with SQL
- Cloud Composer for validation pipelines
- **[ğŸ“– Data Validation Best Practices](https://cloud.google.com/architecture/designing-data-validation-pipelines)** - Validation patterns

## Monitoring and Optimization

### Cloud Monitoring

**Infrastructure and Pipeline Monitoring:**
- Pre-built dashboards for GCP services
- Custom metrics from applications
- Alerting policies with notification channels
- Uptime checks for service availability
- **[ğŸ“– Cloud Monitoring](https://cloud.google.com/monitoring/docs)** - Complete guide
- **[ğŸ“– Dataflow Monitoring](https://cloud.google.com/dataflow/docs/guides/using-cloud-monitoring)** - Pipeline metrics
- **[ğŸ“– BigQuery Monitoring](https://cloud.google.com/bigquery/docs/monitoring)** - Query monitoring
- **[ğŸ“– Pub/Sub Monitoring](https://cloud.google.com/pubsub/docs/monitoring)** - Message queue metrics

### Cloud Logging

**Centralized Log Management:**
- Automatic collection of GCP service logs
- Log sinks to BigQuery for analysis
- Log-based metrics for custom monitoring
- Audit logs for compliance
- **[ğŸ“– Cloud Logging](https://cloud.google.com/logging/docs)** - Logging overview
- **[ğŸ“– Log Sinks](https://cloud.google.com/logging/docs/export)** - Export to BigQuery/Storage
- **[ğŸ“– Audit Logs](https://cloud.google.com/logging/docs/audit)** - Compliance logging
- **[ğŸ“– Log Analytics](https://cloud.google.com/logging/docs/log-analytics)** - SQL-based log analysis

### Performance Optimization

**BigQuery Optimization:**
- Avoid SELECT * - specify columns
- Filter early with WHERE clauses
- Use partitioning and clustering
- Denormalize data when appropriate
- Use approximate aggregation functions
- **[ğŸ“– Query Best Practices](https://cloud.google.com/bigquery/docs/best-practices-performance-compute)** - Query optimization
- **[ğŸ“– SQL Anti-Patterns](https://cloud.google.com/bigquery/docs/best-practices-performance-patterns)** - What to avoid

**Dataflow Optimization:**
- Use windowing to reduce state size
- Tune worker resources (CPU, memory, disk)
- Enable Streaming Engine for streaming jobs
- Use side inputs for broadcast joins
- Batch elements with GroupIntoBatches
- **[ğŸ“– Dataflow Best Practices](https://cloud.google.com/dataflow/docs/guides/best-practices)** - Pipeline optimization
- **[ğŸ“– Pipeline Performance](https://cloud.google.com/dataflow/docs/guides/pipeline-performance)** - Troubleshooting guide

**Bigtable Optimization:**
- Design row keys to avoid hotspots
- Pre-split tables for write-heavy workloads
- Use caching for frequently accessed data
- Monitor key metrics: CPU, storage, latency
- **[ğŸ“– Bigtable Performance Guide](https://cloud.google.com/bigtable/docs/performance)** - Optimization strategies

## Data Migration Strategies

### Migration Patterns

**The 5 Rs of Migration:**
1. **Rehost** - Lift-and-shift (VM migration)
2. **Replatform** - Minor optimizations (Dataproc instead of on-prem Hadoop)
3. **Refactor** - Re-architect for cloud-native (Dataflow, BigQuery)
4. **Retire** - Decommission unnecessary systems
5. **Retain** - Keep on-premises temporarily

**Migration Tools:**
- **[ğŸ“– Data Migration Guide](https://cloud.google.com/architecture/hadoop/hadoop-gcp-migration-overview)** - Hadoop migration
- **[ğŸ“– BigQuery Migration Service](https://cloud.google.com/bigquery/docs/migration-intro)** - SQL translation
- **[ğŸ“– Database Migration Service](https://cloud.google.com/database-migration/docs)** - Database replication

### Hadoop to GCP Migration

**Migration Path:**
- HDFS â†’ Cloud Storage
- MapReduce/Spark â†’ Dataproc or Dataflow
- Hive/Impala â†’ BigQuery
- HBase â†’ Bigtable
- **[ğŸ“– Hadoop to GCP](https://cloud.google.com/architecture/hadoop/hadoop-gcp-migration-data)** - Migration guide
- **[ğŸ“– Dataproc Best Practices](https://cloud.google.com/architecture/hadoop/hadoop-gcp-migration-jobs)** - Job migration

### Data Warehouse Migration

**Migration to BigQuery:**
- Schema translation tools
- Data extraction and loading strategies
- Query translation (Teradata, Redshift, SQL Server)
- Incremental migration approach
- **[ğŸ“– Data Warehouse Migration](https://cloud.google.com/architecture/dw2bq/dw-bq-migration-overview)** - Complete guide
- **[ğŸ“– BigQuery Migration Service](https://cloud.google.com/bigquery/docs/migration-intro)** - Automated translation

## Common Data Architecture Patterns

### Pattern 1: Real-Time Analytics Pipeline

**Architecture:**
- **Ingestion** - Pub/Sub for event streaming
- **Processing** - Dataflow for stream processing and aggregation
- **Storage** - BigQuery for analytics, Bigtable for operational queries
- **Visualization** - Looker or Data Studio for dashboards

**Key Services:**
- Pub/Sub with exactly-once delivery
- Dataflow with Streaming Engine
- BigQuery streaming inserts or Storage Write API
- **[ğŸ“– Streaming Analytics Reference Architecture](https://cloud.google.com/architecture/streaming-analytics)** - Complete pattern

### Pattern 2: Batch ETL Pipeline

**Architecture:**
- **Source** - Cloud Storage (CSV, JSON, Avro, Parquet)
- **Orchestration** - Cloud Composer (Airflow) DAGs
- **Processing** - Dataflow or Dataproc for transformations
- **Destination** - BigQuery for analytics

**Key Services:**
- Cloud Composer for scheduling
- Dataflow for serverless ETL
- BigQuery for data warehouse
- **[ğŸ“– Batch ETL Pattern](https://cloud.google.com/architecture/performing-etl-from-relational-database-into-bigquery)** - ETL architecture

### Pattern 3: Data Lake Architecture

**Architecture:**
- **Raw Zone** - Cloud Storage (original formats)
- **Processed Zone** - Cloud Storage (cleaned, validated)
- **Curated Zone** - Cloud Storage or BigQuery (analytics-ready)
- **Metadata** - Data Catalog for discovery

**Key Services:**
- Cloud Storage for all zones
- Dataflow for processing between zones
- BigQuery external tables for ad-hoc queries
- **[ğŸ“– Data Lake on GCP](https://cloud.google.com/architecture/build-a-data-lake-on-gcp)** - Complete architecture

### Pattern 4: Machine Learning Pipeline

**Architecture:**
- **Data Prep** - Dataflow for feature engineering
- **Training** - Vertex AI for model training
- **Serving** - Vertex AI Endpoints for online prediction
- **Orchestration** - Vertex AI Pipelines or Cloud Composer

**Key Services:**
- BigQuery for feature store
- Vertex AI for ML lifecycle
- Cloud Storage for model artifacts
- **[ğŸ“– MLOps Architecture](https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning)** - ML pipelines

### Pattern 5: Hybrid and Multi-Cloud Data

**Architecture:**
- **Connectivity** - Cloud VPN or Interconnect
- **Data Transfer** - Storage Transfer Service
- **Processing** - Dataproc or Dataflow
- **Multi-Cloud** - Pub/Sub for message routing

**Key Services:**
- Cloud Interconnect for dedicated connectivity
- Transfer Service for cross-cloud data movement
- BigQuery Omni for multi-cloud analytics
- **[ğŸ“– Hybrid Data Processing](https://cloud.google.com/architecture/hybrid-and-multi-cloud-data-processing-patterns)** - Hybrid patterns

## Exam Scenarios and Solutions

### Scenario 1: High-Volume Streaming Data

**Requirements:** Ingest millions of IoT events per second, real-time dashboards

**Solution:**
- Pub/Sub for ingestion (regional Pub/Sub Lite for cost optimization)
- Dataflow for aggregation and windowing
- BigQuery for historical analysis
- Bigtable for low-latency operational queries

**Key Decision:** Use Pub/Sub Lite if regional availability is acceptable for cost savings

### Scenario 2: Batch Analytics Pipeline

**Requirements:** Daily ETL from multiple sources, SQL transformations, reporting

**Solution:**
- Cloud Composer for orchestration
- Dataflow or BigQuery SQL for transformations
- BigQuery for data warehouse
- Materialized views for dashboard performance

**Key Decision:** Use BigQuery SQL for transformations if logic is simple; Dataflow for complex multi-source joins

### Scenario 3: Time-Series Data at Scale

**Requirements:** Store billions of time-series records, low-latency reads

**Solution:**
- Bigtable with reverse timestamp row keys
- Dataflow for real-time ingestion
- Precomputed aggregations in BigQuery for analytics
- App profiles for workload isolation

**Key Decision:** Row key design critical - reverse timestamp to avoid hotspots

### Scenario 4: Cost-Optimized Data Lake

**Requirements:** Store petabytes of data, infrequent access, cost-sensitive

**Solution:**
- Cloud Storage Nearline/Coldline/Archive
- Lifecycle policies for automatic tiering
- BigQuery external tables for ad-hoc queries
- Federated queries to avoid data movement

**Key Decision:** Use Archive class for data accessed < 1/year; external tables to avoid storage duplication

### Scenario 5: Data Governance and Compliance

**Requirements:** PII detection, access control, audit trail, data lineage

**Solution:**
- DLP for automatic PII detection and de-identification
- BigQuery column-level and row-level security
- Data Catalog for metadata and lineage
- VPC Service Controls for data perimeter

**Key Decision:** Combine DLP with BigQuery authorized views for dynamic data masking

### Scenario 6: Machine Learning on Large Datasets

**Requirements:** Train models on TB-scale data, feature management, serving

**Solution:**
- BigQuery for feature engineering with SQL
- BigQuery ML for simple models, Vertex AI for complex
- Feature Store for feature serving
- Vertex AI Endpoints for online prediction

**Key Decision:** Use BigQuery ML if SQL-based features work; Vertex AI for custom TensorFlow/PyTorch

## Exam Tips and Strategy

### Keywords to Watch

**Question Patterns:**
- **"Real-time" or "streaming"** â†’ Pub/Sub + Dataflow + BigQuery/Bigtable
- **"Batch" or "scheduled"** â†’ Cloud Composer + Dataflow/Dataproc + BigQuery
- **"Cost-effective"** â†’ Preemptible VMs, Dataproc autoscaling, BigQuery partitioning, FlexRS
- **"Low latency"** â†’ Bigtable, Memorystore, BigQuery BI Engine
- **"Petabyte-scale analytics"** â†’ BigQuery
- **"Time-series data"** â†’ Bigtable with proper row key design
- **"SQL analytics"** â†’ BigQuery
- **"Complex transformations"** â†’ Dataflow with Apache Beam
- **"Simple transformations"** â†’ BigQuery SQL, Dataprep
- **"Machine learning"** â†’ BigQuery ML (SQL), Vertex AI (custom)
- **"Data discovery"** â†’ Data Catalog
- **"PII protection"** â†’ Cloud DLP
- **"Exactly-once processing"** â†’ Pub/Sub + Dataflow

### Service Selection Decision Trees

**Ingestion Decision:**
```
Streaming data?
â”œâ”€ YES â†’ Pub/Sub â†’ Dataflow â†’ BigQuery/Bigtable
â”‚  â””â”€ Regional and cost-sensitive? â†’ Pub/Sub Lite
â””â”€ NO â†’ Batch ingestion
   â”œâ”€ From SaaS apps â†’ BigQuery Data Transfer Service
   â”œâ”€ From Cloud Storage â†’ BigQuery load job
   â”œâ”€ From other clouds â†’ Storage Transfer Service
   â””â”€ Scheduled ETL â†’ Cloud Composer
```

**Processing Decision:**
```
Processing type?
â”œâ”€ Streaming â†’ Dataflow (Apache Beam)
â”œâ”€ Batch ETL
â”‚  â”œâ”€ Simple SQL â†’ BigQuery SQL
â”‚  â”œâ”€ Visual/no-code â†’ Cloud Data Fusion or Dataprep
â”‚  â””â”€ Complex â†’ Dataflow or Dataproc
â””â”€ Hadoop/Spark existing code â†’ Dataproc
```

**Storage Decision:**
```
Data type?
â”œâ”€ Analytics/SQL â†’ BigQuery
â”œâ”€ Time-series/IoT
â”‚  â”œâ”€ Low latency â†’ Bigtable
â”‚  â””â”€ Analytics â†’ BigQuery with partitioning
â”œâ”€ Relational transactional
â”‚  â”œâ”€ Global scale â†’ Cloud Spanner
â”‚  â””â”€ Regional â†’ Cloud SQL
â”œâ”€ Document/NoSQL â†’ Firestore
â”œâ”€ Object/file â†’ Cloud Storage
â””â”€ Caching â†’ Memorystore
```

### Time Management

- 120 minutes Ã· 50 questions = 2.4 minutes per question
- First pass: Answer confident questions (60 minutes)
- Second pass: Tackle scenario-based questions (45 minutes)
- Final pass: Review flagged questions (15 minutes)

### Common Traps

- âŒ Choosing complex solutions when simple ones work (BigQuery SQL vs Dataflow)
- âŒ Not considering cost constraints (on-demand vs flat-rate, preemptible workers)
- âŒ Ignoring latency requirements (Bigtable vs BigQuery)
- âŒ Poor Bigtable row key design (monotonically increasing = hotspot)
- âŒ Not using partitioning/clustering in BigQuery
- âŒ Forgetting about managed alternatives (Dataflow vs self-managed Spark)
- âŒ Not recognizing when to use external tables vs loading data
- âŒ Mixing up Pub/Sub delivery guarantees

### Study Checklist

**Knowledge Areas:**
- [ ] Understand when to use BigQuery vs Bigtable vs Cloud SQL
- [ ] Know Pub/Sub message delivery guarantees and patterns
- [ ] Can design Dataflow pipelines with windowing and triggers
- [ ] Understand BigQuery partitioning and clustering strategies
- [ ] Know Bigtable row key design patterns
- [ ] Familiar with Cloud Composer DAG creation
- [ ] Understand BigQuery ML capabilities and limitations
- [ ] Know data migration strategies (5 Rs)
- [ ] Understand DLP and data governance tools
- [ ] Can optimize costs for data processing workloads

**Hands-On Skills:**
- [ ] Create streaming pipelines with Pub/Sub and Dataflow
- [ ] Design and optimize BigQuery tables
- [ ] Build batch ETL with Cloud Composer
- [ ] Implement Bigtable schema for time-series data
- [ ] Use BigQuery ML to train and deploy models
- [ ] Configure IAM for data access control
- [ ] Set up data quality validation pipelines
- [ ] Monitor and troubleshoot Dataflow jobs

**Preparation:**
- [ ] Build end-to-end data pipelines on GCP
- [ ] Complete official practice exam (80%+ score target)
- [ ] Review case studies in exam guide
- [ ] Read all linked documentation sections
- [ ] Practice with gcloud, bq, and gsutil CLI tools
- [ ] Understand cost implications of design decisions

---

**Pro Tip:** The Professional Data Engineer exam heavily tests your ability to choose the right service for the use case. Always consider: data volume, latency requirements, cost constraints, processing complexity, and operational overhead. Practice building real pipelines to internalize when each service shines!

**Documentation Count:** This fact sheet contains 100+ embedded documentation links to official Google Cloud documentation.

**Good luck!** This certification demonstrates expert-level data engineering skills on Google Cloud Platform.
